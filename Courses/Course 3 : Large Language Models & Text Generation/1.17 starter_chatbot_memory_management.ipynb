{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "JFJaQyi2-EGC",
    "ExecuteTime": {
     "end_time": "2025-05-12T17:55:22.076426Z",
     "start_time": "2025-05-12T17:55:22.074374Z"
    }
   },
   "source": [
    "import openai\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T17:55:22.091599Z",
     "start_time": "2025-05-12T17:55:22.085899Z"
    }
   },
   "source": [
    "openai.api_key =  \"voc-1297329205126677400847568070f740c72e6.53497051\"\n",
    "openai.api_base = \"https://openai.vocareum.com/v1\" # Remove this if using personal key"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T17:55:22.151917Z",
     "start_time": "2025-05-12T17:55:22.149486Z"
    }
   },
   "source": [
    "# Decoding parameters\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS = 3950  # Increased to simulate LLM with smaller attention window\n",
    "TOP_P = 1.0"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GGXSFlArmKt7",
    "ExecuteTime": {
     "end_time": "2025-05-12T17:55:22.159393Z",
     "start_time": "2025-05-12T17:55:22.157425Z"
    }
   },
   "source": [
    "SYSTEM_PROMPT = \"\"\"You expert at games of chance.\n",
    "End every response with double exclamation points!!\"\"\"\n",
    "\n",
    "USER_NAME = \"User\"\n",
    "AI_NAME = \"AI Assistant\"\n",
    "NEW_INTERACTION_DELIMITER = \"\\n\\n\""
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic4juggvKEB8"
   },
   "source": [
    "# Creating a chat bot with memory\n",
    "Using the basic `openai.Completion` API to understand where the memory \"lives\" in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dVHDhMmZmLHF",
    "ExecuteTime": {
     "end_time": "2025-05-12T17:55:22.173665Z",
     "start_time": "2025-05-12T17:55:22.169323Z"
    }
   },
   "source": [
    "def query_openai(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        top_p=TOP_P,\n",
    "    )\n",
    "    time.sleep(5) # to avoid rate limit errors\n",
    "    if \"error\" in response:\n",
    "        raise openai.InvalidRequestError(response[\"error\"], param=None)\n",
    "    else:\n",
    "        return response.choices[0].text.strip().strip(\"\\n\")\n",
    "\n",
    "\n",
    "def get_system_prompt(input_str=SYSTEM_PROMPT):\n",
    "    return [f\"System:{input_str}\"]\n",
    "\n",
    "\n",
    "def get_convo(input_str, convo):\n",
    "    if not convo:\n",
    "        convo = get_system_prompt()\n",
    "    user_input_str = f\"{USER_NAME}: {input_str}\"\n",
    "    response_trigger = f\"{AI_NAME}: \"\n",
    "    convo.extend([user_input_str, response_trigger])\n",
    "    return convo\n",
    "\n",
    "\n",
    "# This is the function we will be fixing\n",
    "def get_response(input_str, convo, use_simple_truncation, verbose):\n",
    "    \"\"\"\n",
    "    Generate a response from an LLM based on user input_str and conversation history.\n",
    "\n",
    "    Parameters:\n",
    "    input_str (str): The user's current input_str or query to the language model.\n",
    "    convo (list of str): A list representing the history of the conversation.\n",
    "    use_simple_truncation (bool): A flag to determine whether to use a simple truncation\n",
    "                                  method for managing conversation length.\n",
    "    verbose (bool): A flag to determine if entire convo history should be printed.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated response from the language model based on the current input_str and\n",
    "         the conversation history.\n",
    "    \"\"\"\n",
    "    convo = get_convo(input_str, convo)\n",
    "\n",
    "    # Try to prompt model and catch if the prompt exceeds the attention window\n",
    "    first_try = True\n",
    "    atten_window_all_used_up = False\n",
    "    while first_try or atten_window_all_used_up:\n",
    "        # Convo list flattened into string to feed to model\n",
    "        flattened_convo = \"\" # TODO\n",
    "\n",
    "        try:\n",
    "            first_try = False\n",
    "            response = query_openai(flattened_convo)\n",
    "            atten_window_all_used_up = False\n",
    "\n",
    "        except openai.InvalidRequestError as e:\n",
    "            atten_window_all_used_up = True\n",
    "            if verbose:\n",
    "                print(\"** ATTEN_WINDOW ALL USED UP **\")\n",
    "                print(f\"OpenAI Error: {repr(e)}\\n\")\n",
    "\n",
    "            if not convo:\n",
    "                return [\n",
    "                    \"Our Error: System prompt is using up too many tokens of the attention window\"\n",
    "                ]\n",
    "\n",
    "            # We can recover from over-allocation of atten_window by removing \n",
    "            # components from history.\n",
    "            if use_simple_truncation:\n",
    "                # Just remove oldest element in convo\n",
    "                convo = []  # TODO\n",
    "\n",
    "            else:\n",
    "                # Remove the oldest User or AI convo turn, while retaining \n",
    "                # system prompt\n",
    "                convo = []  # TODO\n",
    "\n",
    "    # Add the LLM response to the response_trigger\n",
    "    convo[-1] += response\n",
    "    if verbose:\n",
    "        print(NEW_INTERACTION_DELIMITER.join(convo))\n",
    "    else:\n",
    "        print(f\"{USER_NAME}: {input_str}\")\n",
    "        print(f\"{AI_NAME}: {response}\")\n",
    "\n",
    "    return convo\n",
    "\n",
    "\n",
    "def chat(user_query, convo=[], use_simple_truncation=False, verbose=False):\n",
    "    convo = get_response(user_query, convo, use_simple_truncation, verbose)\n",
    "    return convo"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC6CGoDKK5l9"
   },
   "source": [
    "## Testing our Chat bot"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7MFIxTMRmpL0",
    "ExecuteTime": {
     "end_time": "2025-05-12T17:55:22.179857Z",
     "start_time": "2025-05-12T17:55:22.178152Z"
    }
   },
   "source": [
    "user_inputs = [\n",
    "    \"What cards game has the best odds of winning?\",\n",
    "    \"What are the odds of winning it?\",\n",
    "    \"What is the best hand to be dealt?\",\n",
    "    \"What is the next most likely game to win?\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0p61acSK-bW"
   },
   "source": [
    "### Simple convo truncation method\n",
    "Model remembers prior convo but **forgets to** always ends in exclamation points!!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Udi8FpJW8Hf",
    "outputId": "03369e5f-527a-4f7f-fb91-1082ae7d4ed2",
    "ExecuteTime": {
     "end_time": "2025-05-12T17:55:22.284490Z",
     "start_time": "2025-05-12T17:55:22.184202Z"
    }
   },
   "source": [
    "convo = []\n",
    "verbose = False\n",
    "simple_truncation = True\n",
    "for i, input in enumerate(user_inputs):\n",
    "    print(f\"**** Convo turn {i} ****\")\n",
    "    convo = chat(\n",
    "        input, convo=convo, use_simple_truncation=simple_truncation, verbose=verbose\n",
    "    )\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Convo turn 0 ****\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'InvalidRequestError'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAPIRemovedInV1\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 56\u001B[39m, in \u001B[36mget_response\u001B[39m\u001B[34m(input_str, convo, use_simple_truncation, verbose)\u001B[39m\n\u001B[32m     55\u001B[39m first_try = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m response = \u001B[43mquery_openai\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflattened_convo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     57\u001B[39m atten_window_all_used_up = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 2\u001B[39m, in \u001B[36mquery_openai\u001B[39m\u001B[34m(prompt)\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery_openai\u001B[39m(prompt):\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m     response = \u001B[43mopenai\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletion\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgpt-3.5-turbo-instruct\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[43mTEMPERATURE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mMAX_TOKENS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m=\u001B[49m\u001B[43mTOP_P\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m     time.sleep(\u001B[32m5\u001B[39m) \u001B[38;5;66;03m# to avoid rate limit errors\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/openai/lib/_old_api.py:39\u001B[39m, in \u001B[36mAPIRemovedInV1Proxy.__call__\u001B[39m\u001B[34m(self, *_args, **_kwargs)\u001B[39m\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *_args: Any, **_kwargs: Any) -> Any:\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m APIRemovedInV1(symbol=\u001B[38;5;28mself\u001B[39m._symbol)\n",
      "\u001B[31mAPIRemovedInV1\u001B[39m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, \u001B[38;5;28minput\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(user_inputs):\n\u001B[32m      5\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m**** Convo turn \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m ****\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m     convo = \u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvo\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_simple_truncation\u001B[49m\u001B[43m=\u001B[49m\u001B[43msimple_truncation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m     \u001B[38;5;28mprint\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 93\u001B[39m, in \u001B[36mchat\u001B[39m\u001B[34m(user_query, convo, use_simple_truncation, verbose)\u001B[39m\n\u001B[32m     92\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mchat\u001B[39m(user_query, convo=[], use_simple_truncation=\u001B[38;5;28;01mFalse\u001B[39;00m, verbose=\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m     convo = \u001B[43mget_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_simple_truncation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m convo\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 59\u001B[39m, in \u001B[36mget_response\u001B[39m\u001B[34m(input_str, convo, use_simple_truncation, verbose)\u001B[39m\n\u001B[32m     56\u001B[39m     response = query_openai(flattened_convo)\n\u001B[32m     57\u001B[39m     atten_window_all_used_up = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[43mopenai\u001B[49m\u001B[43m.\u001B[49m\u001B[43mInvalidRequestError\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     60\u001B[39m     atten_window_all_used_up = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     61\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m verbose:\n",
      "\u001B[31mAttributeError\u001B[39m: module 'openai' has no attribute 'InvalidRequestError'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqcax0zmLs9X"
   },
   "source": [
    "#### What is happening?\n",
    "Peaking under hood to see the full conversation as it grows\n",
    "\n",
    "NOTE: In **convo turn 3** we remove the system prompt to avoid exceeding the LLM attention window\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58SqyIARWT8c",
    "outputId": "984a59ef-495e-457a-e54f-ddb3e3b88352"
   },
   "source": [
    "convo = []\n",
    "verbose = True\n",
    "simple_truncation = True\n",
    "for i, input in enumerate(user_inputs):\n",
    "    print(f\"**** Convo turn {i} ****\")\n",
    "    convo = chat(\n",
    "        input, convo=convo, use_simple_truncation=simple_truncation, verbose=verbose\n",
    "    )\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDY-XBnUNKUL"
   },
   "source": [
    "### Retaining System prompt truncation method\n",
    "Model remembers most of prior convo and **remembers to** always ends in exclamation points!!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHJnH0YLJKMX",
    "outputId": "5eea21fe-e65c-4628-a43b-b2fe4bbb3093"
   },
   "source": [
    "convo = []\n",
    "verbose = False\n",
    "for i, input in enumerate(user_inputs):\n",
    "    print(f\"**** Convo turn {i} ****\")\n",
    "    convo = chat(input, convo=convo, verbose=verbose)\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBRZFXA6MltU"
   },
   "source": [
    "#### What is happening?\n",
    "Peaking under hood to see full convo as it grows\n",
    "\n",
    "NOTE: In **convo turn 3** we remove the oldest **convo turns**, but retain the system prompt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbSxLx9_M4lw",
    "outputId": "7f1eda93-8c09-4cae-8a4a-e47333e49105"
   },
   "source": [
    "convo = []\n",
    "verbose = True\n",
    "for i, input in enumerate(user_inputs):\n",
    "    print(f\"**** Convo turn {i} ****\")\n",
    "    convo = chat(input, convo=convo, verbose=verbose)\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOb/Rz98rOFMFcM/oGGqMP4",
   "include_colab_link": true,
   "mount_file_id": "15ObT0WPB-oL1W-p7iT5JP2oHwSKDmWBb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
