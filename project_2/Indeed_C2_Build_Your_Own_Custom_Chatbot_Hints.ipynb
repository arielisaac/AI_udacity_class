{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4LAfvuP7TK6"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: DO NOT EXECUTE THIS CELL IF YOU'RE USING UDACITY WORKSPACE.\n",
        "\n",
        "# Making sure we are using the appropriate version of OpenAI library and installing tiktoken if it is not already installed\n",
        "# Restart the kernel/session after installation completes\n",
        "!pip install -q openai==0.27.7\n",
        "!pip install -q tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Choice\n",
        "\n",
        "I have chosen the ... dataset. (Explain your choice)"
      ],
      "metadata": {
        "id": "nITaBZp97_7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# DO NOT MODIFY THIS CELL\n",
        "# ===============================\n",
        "import getpass\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from openai.embeddings_utils import distances_from_embeddings"
      ],
      "metadata": {
        "id": "70Yo5eJy8D3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# API Key Configuration (DO NOT MODIFY)\n",
        "# ===============================\n",
        "\n",
        "openai.api_base = \"https://openai.vocareum.com/v1\"\n",
        "\n",
        "# This function is complete and should not be modified.\n",
        "def get_openai_api_key():\n",
        "    key = getpass.getpass(\"Enter OpenAI API key (input hidden): \").strip()\n",
        "    while not key:\n",
        "        print(\"API key cannot be empty!\")\n",
        "        key = getpass.getpass(\"Enter OpenAI API key (input hidden): \").strip()\n",
        "\n",
        "    print(f\"API key configured (last 4 chars): ****{key[-4:]}\")\n",
        "    return key\n",
        "\n",
        "openai.api_key = get_openai_api_key()"
      ],
      "metadata": {
        "id": "xT1BnEFW8inM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Dataset & Embedding Functions\n",
        "# ===============================\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    # TODO: Replace the following line with code that extracts the text from the appropriate column.\n",
        "    # For example, if your text data is in a column named 'Trends', then you might use:\n",
        "    #    df['text'] = df['Trends']\n",
        "    # Otherwise, update 'Trends' to the correct column name.\n",
        "    # >>>>> Hint #1 <<<<<\n",
        "    # df['text'] = df['Trends']\n",
        "    df['text'] = None  # <-- Replace this placeholder with the correct extraction code.\n",
        "    return df[['text']]\n",
        "\n",
        "def generate_embeddings(df, embedding_model_name=\"text-embedding-ada-002\", batch_size=1):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        response = openai.Embedding.create(\n",
        "            input=df.iloc[i:i + batch_size][\"text\"].tolist(),\n",
        "            engine=embedding_model_name\n",
        "        )\n",
        "        # TODO: Extract each embedding from response[\"data\"] and append it to the embeddings list.\n",
        "        # >>>>> Hint #2 <<<<<\n",
        "        # embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n",
        "        embeddings.extend( [None] )  # <-- Replace this placeholder with your extraction logic.\n",
        "    df[\"embeddings\"] = embeddings\n",
        "    return df\n",
        "\n",
        "def save_embeddings(df, output_file):\n",
        "    # TODO: Save the DataFrame to a CSV file. Make sure you set the parameter to avoid saving row indices.\n",
        "    # >>>>> Hint #3 <<<<<\n",
        "    # df.to_csv(output_file, index=False)\n",
        "    df.to_csv(output_file)  # <-- Update this call by adding the appropriate parameter.\n",
        "\n",
        "def load_embeddings(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    # TODO: Convert the string representation in the 'embeddings' column back into numpy arrays.\n",
        "    # >>>>> Hint #4 <<<<<\n",
        "    # df[\"embeddings\"] = df[\"embeddings\"].apply(eval).apply(np.array)\n",
        "    df[\"embeddings\"] = df[\"embeddings\"].apply(eval)  # <-- Complete the conversion as needed.\n",
        "    return df\n",
        "\n",
        "def get_relevant_rows(question, df, embedding_model_name=\"text-embedding-ada-002\", top_n=10):\n",
        "    question_embedding = openai.Embedding.create(\n",
        "        model=embedding_model_name,\n",
        "        input=question\n",
        "    )['data'][0]['embedding']\n",
        "\n",
        "    df_copy = df.copy()\n",
        "    # TODO: Compute the cosine distances between the question embedding and each text embedding.\n",
        "    # Hint: Use distances_from_embeddings() with the appropriate parameters.\n",
        "    # >>>>> Hint #5 <<<<<\n",
        "    # df_copy['distance'] = distances_from_embeddings(question_embedding, df_copy['embeddings'].values, distance_metric=\"cosine\")\n",
        "    df_copy['distance'] = None  # <-- Replace this placeholder with the proper distance computation.\n",
        "\n",
        "    return df_copy.nsmallest(top_n, 'distance')"
      ],
      "metadata": {
        "id": "dDJTLd_U8l9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Prompt Creation & Answering\n",
        "# ===============================\n",
        "\n",
        "def create_prompt(question, df, max_token_count=1500):\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    prompt_template = \"\"\"\n",
        "    Answer the question based on the context below. If the question can't be answered based on the context, say \"I don't know.\"\n",
        "\n",
        "    Context: {}\n",
        "\n",
        "    ---\n",
        "\n",
        "    Question: {}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    # TODO: Compute the initial token count using the prompt_template and question.\n",
        "    # >>>>> Hint #1 <<<<<\n",
        "    # current_token_count = len(tokenizer.encode(prompt_template)) + len(tokenizer.encode(question))\n",
        "    current_token_count = 0  # <-- Replace with the actual token count calculation.\n",
        "\n",
        "    context = []\n",
        "    for text in df[\"text\"].values:\n",
        "        tokens_in_text = len(tokenizer.encode(text))\n",
        "        if current_token_count + tokens_in_text <= max_token_count:\n",
        "            context.append(text)\n",
        "            # TODO: Update the current_token_count by adding tokens_in_text.\n",
        "            # >>>>> Hint #2 <<<<<\n",
        "            # current_token_count += tokens_in_text\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)\n",
        "\n",
        "def get_openai_answer(prompt, max_answer_tokens=150):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=max_answer_tokens\n",
        "        )\n",
        "        # TODO: Extract and return the generated answer text from the response.\n",
        "        # >>>>> Hint #3 <<<<<\n",
        "        # return response[\"choices\"][0][\"text\"].strip()\n",
        "        return None  # <-- Replace this with the correct extraction code.\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return \"An error occurred.\""
      ],
      "metadata": {
        "id": "8GdKMnox9BjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Question Answering Functions\n",
        "# ===============================\n",
        "\n",
        "def answer_basic_question(question, max_answer_tokens=150):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=question,\n",
        "            max_tokens=max_answer_tokens\n",
        "        )\n",
        "        # TODO: Extract and return the answer from the response.\n",
        "        # >>>>> Hint #1 <<<<<\n",
        "        # return response[\"choices\"][0][\"text\"].strip()\n",
        "        return None  # <-- Replace this placeholder with your extraction logic.\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return \"An error occurred.\"\n",
        "\n",
        "def answer_question_with_context(question, df, max_prompt_tokens=1500, max_answer_tokens=150, top_n=10):\n",
        "    relevant_rows = get_relevant_rows(question, df, top_n=top_n)\n",
        "    # TODO: Construct a combined prompt using the relevant rows and the question.\n",
        "    # >>>>> Hint #2 <<<<<\n",
        "    # prompt = create_prompt(question, relevant_rows, max_token_count=max_prompt_tokens)\n",
        "    prompt = None  # <-- Replace this placeholder with a call to create_prompt.\n",
        "\n",
        "    # TODO: Generate and return the answer using the combined prompt.\n",
        "    # >>>>> Hint #3 <<<<<\n",
        "    # return get_openai_answer(prompt, max_answer_tokens=max_answer_tokens)\n",
        "    return None  # <-- Replace this placeholder with the proper call."
      ],
      "metadata": {
        "id": "lac9V6PO9Jen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Main Function\n",
        "# ===============================\n",
        "\n",
        "def main():\n",
        "    # Load the dataset from the CSV file.\n",
        "    # >>>>> Hint #1 <<<<<\n",
        "    # df = load_dataset(\"./2023_fashion_trends.csv\")\n",
        "    df = load_dataset(None)  # <-- Verify and adjust if needed.\n",
        "\n",
        "    # Generate embeddings and save them to a CSV file.\n",
        "    df = generate_embeddings(df)\n",
        "    # TODO: Save the embeddings to a CSV file (ensure proper parameters).\n",
        "    # >>>>> Hint #2 <<<<<\n",
        "    # save_embeddings(df, \"./embeddings_with_vectors.csv\")\n",
        "    save_embeddings(None)  # <-- Verify and adjust if needed.\n",
        "\n",
        "    # TODO: Load the embeddings back from the CSV file.\n",
        "    # >>>>> Hint #3 <<<<<\n",
        "    # df = load_embeddings(\"./embeddings_with_vectors.csv\")\n",
        "    df = load_embeddings(None)  # <-- Verify and adjust if needed.\n",
        "\n",
        "    # Example Question 1\n",
        "    question1 = \"What are the popular fashion trends in 2023?\"    # <-- Replace with your own question depending on chosen dataset.\n",
        "    basic_answer1 = answer_basic_question(question1)\n",
        "    custom_answer1 = answer_question_with_context(question1, df)\n",
        "\n",
        "    # Example Question 2\n",
        "    question2 = \"What is the trend for oversized bags?\"  # <-- Replace with your own question depending on chosen dataset.\n",
        "    basic_answer2 = answer_basic_question(question2)\n",
        "    custom_answer2 = answer_question_with_context(question2, df)\n",
        "\n",
        "    # TODO: Print the results for both questions.\n",
        "    # >>>>> Hint #4 <<<<<\n",
        "    # print(f\"Question 1: {question1}\\nBasic Answer: {basic_answer1}\\n\\nCustom Answer: {custom_answer1}\\n\\n\")\n",
        "    # print(f\"Question 2: {question2}\\nBasic Answer: {basic_answer2}\\n\\nCustom Answer: {custom_answer2}\\n\\n\")\n",
        "    print(\"<< Print statements here >>\")  # <-- Replace with your final print statements."
      ],
      "metadata": {
        "id": "6CdQesrr9XEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Execution\n",
        "# ===============================\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bRt0oHI_9xj4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}