{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS0aUejsau1h"
   },
   "source": [
    "# Lightweight Fine-Tuning Project (Hints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KR-eMsKa7NL"
   },
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: DoRA (Weight-Decomposed Low-Rank Adaptation)\n",
    "* Model: distilbert-base-uncased\n",
    "* Evaluation approach: Accuracy\n",
    "* Fine-tuning dataset: sms_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAFDyH9Ea_ox"
   },
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FVXgHeK78zJF",
    "ExecuteTime": {
     "end_time": "2025-04-28T04:40:31.000389Z",
     "start_time": "2025-04-28T04:40:23.255623Z"
    }
   },
   "source": [
    "# Installing required libraries\n",
    "# Note: After running this cell, restart the kernel to ensure the new packages are properly loaded.\n",
    "# Instructions to restart the kernel:\n",
    "# 1. Go to the top menu in Jupyter Notebook and click on \"Kernel\".\n",
    "# 2. From the dropdown, select \"Restart\".\n",
    "# 3. Confirm the restart when prompted.\n",
    "# 4. Wait for the kernel to restart (indicated by the kernel icon becoming active again).\n",
    "# 5. Once the kernel is restarted, continue executing cells from the next one onwards.\n",
    "\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U peft"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zJqxvORi9B-X",
    "ExecuteTime": {
     "end_time": "2025-04-28T04:40:35.162056Z",
     "start_time": "2025-04-28T04:40:31.007400Z"
    }
   },
   "source": [
    "# Suppressing unrelated warnings\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "\n",
    "# Necessary imports\n",
    "from datasets import load_dataset\n",
    "from peft import (AutoPeftModelForSequenceClassification,\n",
    "                  LoraConfig,\n",
    "                  get_peft_model,\n",
    "                  TaskType)\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding,\n",
    "                          Trainer,\n",
    "                          TrainingArguments)\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XWTZTkR-9LgD",
    "ExecuteTime": {
     "end_time": "2025-04-28T04:40:40.665752Z",
     "start_time": "2025-04-28T04:40:35.248876Z"
    }
   },
   "source": [
    "# Loading the sms_spam dataset\n",
    "# Dataset here: https://huggingface.co/datasets/sms_spam\n",
    "# The sms_spam dataset only has a train split, so we use the train_test_split method to split it into train and test\n",
    "dataset = load_dataset(\"sms_spam\", split=\"train\").train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=23\n",
    ")\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# View the dataset characteristics\n",
    "dataset[\"train\"]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label'],\n",
       "    num_rows: 4459\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j77qHzGX9aRY",
    "ExecuteTime": {
     "end_time": "2025-04-28T04:40:40.673336Z",
     "start_time": "2025-04-28T04:40:40.670886Z"
    }
   },
   "source": [
    "# TODO: Load the tokenizer for the \"distilbert-base-uncased\" model.\n",
    "# Hint 1: Use AutoTokenizer.from_pretrained(\"<model-name>\").\n",
    "# Hint 2: The tokenizer will help convert text to tokens that the model can process.\n",
    "tokenizer = None  # Replace None with the appropriate code.\n",
    "\n",
    "# TODO: Tokenize the dataset using the loaded tokenizer.\n",
    "# Hint 1: Use the map() function with a lambda to apply tokenization to the \"sms\" column.\n",
    "# Hint 2: Set truncation=True to ensure the sequences fit within the model's maximum input length.\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = None  # Replace None with the appropriate code."
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9v9Pepou9bol"
   },
   "source": [
    "<details>\n",
    "<summary>Click to see the answer</summary>\n",
    "\n",
    "### Solution for Tokenizer Initialization and Tokenization\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"sms\"], truncation=True), batched=True\n",
    "    )\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wh2b6N6G9f7l",
    "ExecuteTime": {
     "end_time": "2025-04-28T04:40:40.689931Z",
     "start_time": "2025-04-28T04:40:40.688088Z"
    }
   },
   "source": [
    "# TODO: Implement a function to compute accuracy from predictions and labels.\n",
    "# Hint 1: Use np.argmax to find the predicted class from the logits.\n",
    "# Hint 2: Compare the predicted classes with the true labels to calculate accuracy.\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = None  # Replace None with np.argmax to get predicted classes.\n",
    "    return {\"accuracy\": None}  # Replace None to compute the mean of correct predictions."
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84tNWN1b9s0E"
   },
   "source": [
    "<details>\n",
    "<summary>Click to see the answer</summary>\n",
    "\n",
    "### Solution for Metrics Function\n",
    "```python\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WEUcg9F09toX",
    "ExecuteTime": {
     "end_time": "2025-04-28T04:40:40.906160Z",
     "start_time": "2025-04-28T04:40:40.717022Z"
    }
   },
   "source": [
    "# Loading distilbert-base-uncased for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"not spam\", 1: \"spam\"},\n",
    "    label2id={\"not spam\": 0, \"spam\": 1},\n",
    ")\n",
    "\n",
    "# Define a function to print the trainable parameters of the model\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print()\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trainable model parameters: 66955010\n",
      "all model parameters: 66955010\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tx_JPg11-CEB",
    "ExecuteTime": {
     "end_time": "2025-04-28T04:40:40.911307Z",
     "start_time": "2025-04-28T04:40:40.908935Z"
    }
   },
   "source": [
    "# TODO: Create the Trainer class instance to evaluate the pre-trained model on the test set.\n",
    "# Hint 1: Trainer's eval_dataset argument takes an evaluation dataset (i.e. tokenized_dataset[\"test\"]) and computes_metrics argument will take the compute_metrics function callable defined above.\n",
    "# Hint 2: args argument of Trainer will be this --> args=TrainingArguments(output_dir=\"./result-distilbert-base\", per_device_eval_batch_size=4, report_to=\"none\")\n",
    "print(\"Evaluating the model before fine-tuning...\")\n",
    "trainer = None  # Replace with the code given in Hint 2 above\n",
    "pre_finetune_eval = None  # Replace None with the appropriate trainer.evaluate() call.\n",
    "print(pre_finetune_eval)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model before fine-tuning...\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FO3T98c5-G8a"
   },
   "source": [
    "<details>\n",
    "<summary>Click to see the answer</summary>\n",
    "\n",
    "### Solution for Model Evaluation Before Fine-Tuning\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./result-distilbert-base\",\n",
    "        per_device_eval_batch_size=4,\n",
    "        report_to=\"none\"\n",
    "    ),\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "pre_finetune_eval = trainer.evaluate()\n",
    "print(pre_finetune_eval)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nScvRlTUbHqm"
   },
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IbVGrvQN-HoH",
    "ExecuteTime": {
     "end_time": "2025-04-28T04:40:41.250185Z",
     "start_time": "2025-04-28T04:40:40.916168Z"
    }
   },
   "source": [
    "# TODO: Complete the LoRA configuration.\n",
    "# Hint 1: Use r=16, lora_alpha=16, and lora_dropout=0.05 as default values.\n",
    "# Hint 2: Specify target_modules where LoRA is applied (e.g., MultiHeadAttention layers) i.e. target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"]\n",
    "# Hint 3: Specify modules_to_save i.e. modules where full fine-tuning is done; for e.g. modules_to_save=[\"classifier\"]\n",
    "# Hint 4: Use use_dora=True\n",
    "# Hint 5: Use task_type=TaskType.SEQ_CLS for sequence classification tasks.\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=None,  # Replace None with the list of target modules i.e. modules targeted for Parameter Efficient Fine-Tuning.\n",
    "    modules_to_save=None, # Replace None with the list of modules for which full fine-tuning will be done.\n",
    "    bias='none',\n",
    "    task_type=None  # Replace None with the correct task type.\n",
    ")\n",
    "\n",
    "# Get PEFT model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Reduced trainable parameters\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please specify `target_modules` in `peft_config`",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m      7\u001B[39m lora_config = LoraConfig(\n\u001B[32m      8\u001B[39m     r=\u001B[32m16\u001B[39m,\n\u001B[32m      9\u001B[39m     lora_alpha=\u001B[32m16\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     14\u001B[39m     task_type=\u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# Replace None with the correct task type.\u001B[39;00m\n\u001B[32m     15\u001B[39m )\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# Get PEFT model\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m peft_model = \u001B[43mget_peft_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlora_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# Reduced trainable parameters\u001B[39;00m\n\u001B[32m     21\u001B[39m \u001B[38;5;28mprint\u001B[39m(print_number_of_trainable_model_parameters(peft_model))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/peft/mapping_func.py:113\u001B[39m, in \u001B[36mget_peft_model\u001B[39m\u001B[34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    110\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m PeftMixedModel(model, peft_config, adapter_name=adapter_name)\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m peft_config.task_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m peft_config.is_prompt_learning:\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPeftModel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    114\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    116\u001B[39m \u001B[43m        \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    117\u001B[39m \u001B[43m        \u001B[49m\u001B[43mautocast_adapter_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mautocast_adapter_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    119\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m peft_config.is_prompt_learning:\n\u001B[32m    122\u001B[39m     peft_config = _prepare_prompt_learning_config(peft_config, model_config)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/peft/peft_model.py:132\u001B[39m, in \u001B[36mPeftModel.__init__\u001B[39m\u001B[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    130\u001B[39m     ctx = init_empty_weights \u001B[38;5;28;01mif\u001B[39;00m low_cpu_mem_usage \u001B[38;5;28;01melse\u001B[39;00m nullcontext\n\u001B[32m    131\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx():\n\u001B[32m--> \u001B[39m\u001B[32m132\u001B[39m         \u001B[38;5;28mself\u001B[39m.base_model = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[38;5;28mself\u001B[39m.set_additional_trainable_modules(peft_config, adapter_name)\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.base_model, \u001B[33m\"\u001B[39m\u001B[33m_cast_adapter_dtype\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/peft/tuners/lora/model.py:142\u001B[39m, in \u001B[36mLoraModel.__init__\u001B[39m\u001B[34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    141\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, config, adapter_name, low_cpu_mem_usage: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m142\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:180\u001B[39m, in \u001B[36mBaseTuner.__init__\u001B[39m\u001B[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    178\u001B[39m \u001B[38;5;28mself\u001B[39m._pre_injection_hook(\u001B[38;5;28mself\u001B[39m.model, \u001B[38;5;28mself\u001B[39m.peft_config[adapter_name], adapter_name)\n\u001B[32m    179\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m peft_config != PeftType.XLORA \u001B[38;5;129;01mor\u001B[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001B[32m--> \u001B[39m\u001B[32m180\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minject_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    182\u001B[39m \u001B[38;5;66;03m# Copy the peft_config in the injected model.\u001B[39;00m\n\u001B[32m    183\u001B[39m \u001B[38;5;28mself\u001B[39m.model.peft_config = \u001B[38;5;28mself\u001B[39m.peft_config\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:433\u001B[39m, in \u001B[36mBaseTuner.inject_adapter\u001B[39m\u001B[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    429\u001B[39m _has_modules_to_save = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    431\u001B[39m model_config = \u001B[38;5;28mself\u001B[39m.get_model_config(model)\n\u001B[32m--> \u001B[39m\u001B[32m433\u001B[39m peft_config = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_prepare_adapter_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    435\u001B[39m \u001B[38;5;28mself\u001B[39m._prepare_model(peft_config, model)\n\u001B[32m    436\u001B[39m key_list = [key \u001B[38;5;28;01mfor\u001B[39;00m key, _ \u001B[38;5;129;01min\u001B[39;00m model.named_modules()]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/peft/tuners/lora/model.py:499\u001B[39m, in \u001B[36mLoraModel._prepare_adapter_config\u001B[39m\u001B[34m(peft_config, model_config)\u001B[39m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m peft_config.target_modules \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    498\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m model_config[\u001B[33m\"\u001B[39m\u001B[33mmodel_type\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING:\n\u001B[32m--> \u001B[39m\u001B[32m499\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mPlease specify `target_modules` in `peft_config`\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    500\u001B[39m     peft_config.target_modules = \u001B[38;5;28mset\u001B[39m(\n\u001B[32m    501\u001B[39m         TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[model_config[\u001B[33m\"\u001B[39m\u001B[33mmodel_type\u001B[39m\u001B[33m\"\u001B[39m]]\n\u001B[32m    502\u001B[39m     )\n\u001B[32m    503\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m peft_config\n",
      "\u001B[31mValueError\u001B[39m: Please specify `target_modules` in `peft_config`"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY8HRnB6_ASP"
   },
   "source": [
    "<details>\n",
    "<summary>Click to see the answer</summary>\n",
    "\n",
    "### Solution for LoRA Configuration\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],\n",
    "    modules_to_save=[\"classifier\"],\n",
    "    bias='none',\n",
    "    use_dora=True,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sq2sm5DC_GYI"
   },
   "outputs": [],
   "source": [
    "# Training and evaluating the model prepared for PEFT\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./result-distilbert-lora\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_steps=1,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\"\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lTVwryM_XzI"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model after fine-tuning\n",
    "print(\"Evaluating the model after fine-tuning...\")\n",
    "post_finetune_eval = trainer.evaluate()\n",
    "print(post_finetune_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzywX_xD_pVn"
   },
   "outputs": [],
   "source": [
    "# TODO: Save the fine-tuned model to a directory.\n",
    "# Hint: Use model.save_pretrained(\"<directory-name>\").\n",
    "peft_model.save_pretrained(None)  # Replace None with the directory path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQZCfbX0_s3H"
   },
   "source": [
    "<details>\n",
    "<summary>Click to see the answer</summary>\n",
    "\n",
    "### Solution for Saving the Model\n",
    "```python\n",
    "peft_model.save_pretrained(\"distilbert-lora\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aL2hum9bY3G"
   },
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yl7A8f6C_wUv"
   },
   "outputs": [],
   "source": [
    "# TODO: Load the saved model from the directory.\n",
    "# Hint: Use AutoPeftModelForSequenceClassification.from_pretrained(\"<directory-name>\").\n",
    "lora_model = None  # Replace None with the appropriate code to load the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAdylTntACmB"
   },
   "source": [
    "<details>\n",
    "<summary>Click to see the answer</summary>\n",
    "\n",
    "### Solution for Loading the Model\n",
    "```python\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"distilbert-lora\")\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6fe3q2rADSN"
   },
   "outputs": [],
   "source": [
    "# Evaluate the loaded fine-tuned model\n",
    "loaded_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./result-distilbert-lora\",\n",
    "        per_device_eval_batch_size=4,\n",
    "        report_to=\"none\"\n",
    "    ),\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Evaluating the loaded fine-tuned model...\")\n",
    "loaded_model_eval = loaded_trainer.evaluate()\n",
    "print(loaded_model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4m0AzDyuFXc1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
