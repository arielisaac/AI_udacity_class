{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arielisaac/AI_udacity_class/blob/main/project_1/LightweightFineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35354cd",
      "metadata": {
        "id": "f35354cd"
      },
      "source": [
        "# Lightweight Fine-Tuning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560fb3ff",
      "metadata": {
        "id": "560fb3ff"
      },
      "source": [
        "TODO: In this cell, describe your choices for each of the following\n",
        "\n",
        "* PEFT technique:\n",
        "* Model:\n",
        "* Evaluation approach:\n",
        "* Fine-tuning dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8d76bb",
      "metadata": {
        "id": "de8d76bb"
      },
      "source": [
        "## Loading and Evaluating a Foundation Model\n",
        "\n",
        "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
      ]
    },
    {
      "cell_type": "code",
      "id": "f551c63a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-22T18:25:45.388236Z",
          "start_time": "2025-04-22T18:25:40.779654Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f551c63a",
        "outputId": "78bd816b-1cca-4613-82d6-7e8918af4e81"
      },
      "source": [
        "!pip install transformers datasets peft accelerate evaluate optuna\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A few Configurations"
      ],
      "metadata": {
        "id": "WEr2i7xnc3cA"
      },
      "id": "WEr2i7xnc3cA"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Disable torch.compile for stability\n",
        "torch._dynamo.config.disable = True\n",
        "\n",
        "# This is to improve performance on M1 Macs\n",
        "def get_best_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    else:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "device = get_best_device()\n",
        "\n",
        "# ModernBERT blog: https://www.answer.ai/posts/2024-12-19-modernbert.html\n",
        "# Philipp Schmid's blog on Fine-Tuning open LLMs: https://www.philschmid.de/fine-tune-llms-in-2025\n",
        "\n",
        "# https://learn.udacity.com/nanodegrees/nd101-ent-ai-swe-indeed/parts/cd13303/lessons/786df5de-95ad-4e0d-be51-cc8a1c1e40fe/concepts/ed4cd691-b999-454e-b715-a603fb2aeeb5?lesson_tab=lesson\n",
        "from peft import LoraConfig\n",
        "config = LoraConfig()\n",
        "# https://huggingface.co/docs/peft/main/en/conceptual_guides/lora"
      ],
      "metadata": {
        "id": "6A2Q-abqUJUq"
      },
      "id": "6A2Q-abqUJUq",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset"
      ],
      "metadata": {
        "id": "8ds_cnNrc_5N"
      },
      "id": "8ds_cnNrc_5N"
    },
    {
      "cell_type": "code",
      "id": "4935cb4d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-27T02:33:23.991633Z",
          "start_time": "2025-04-27T02:33:16.503879Z"
        },
        "id": "4935cb4d",
        "outputId": "2e3741ff-27c3-497a-d46f-9279fff00232",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# # Another dataset availabe: sms_spam\n",
        "# sms_spam_dataset = load_dataset(\"sms_spam\", split=[\"train\"])[0]\n",
        "# print(sms_spam_dataset)\n",
        "# # Split the dataset into train and test sets (assuming `dataset` is a Hugging Face Dataset object)\n",
        "# sms_spam_dataset_train_test_split = sms_spam_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "# test_dataset = sms_spam_dataset_train_test_split['test']\n",
        "# print(sms_spam_dataset[:5])\n",
        "# \"\"\"\n",
        "# Convert the dataset to a pandas DataFrame\n",
        "# \"\"\"\n",
        "# import pandas as pd\n",
        "\n",
        "# # Convert the entire dataset to a pandas DataFrame\n",
        "# df = pd.DataFrame(sms_spam_dataset)\n",
        "# # Display the first 10 rows\n",
        "# print(df.head(10))\n",
        "\n",
        "# # Or for more detailed information\n",
        "# display(df.head(10))  # Works in Jupyter notebooks\n",
        "# display(df.describe())  # Works in Jupyter notebooks\n",
        "\n",
        "# for entry in dataset.select(range(3)):\n",
        "#     sms = entry[\"sms\"]\n",
        "#     label = entry[\"label\"]\n",
        "#     print(f\"label={label}, sms={sms}\")\n",
        "\n",
        "\n",
        "# Load the train and test splits of the imdb dataset\n",
        "splits = [\"train\", \"test\"]\n",
        "ds_imdb = {split: ds for split, ds in zip(splits, load_dataset(\"imdb\", split=splits))}\n",
        "\n",
        "\n",
        "# Thin out the dataset to make it run faster for this example\n",
        "for split in splits:\n",
        "    ds_imdb[split] = ds_imdb[split].shuffle(seed=42).select(range(500))\n",
        "print(ds_imdb)\n",
        "\n",
        "# Count the number of positive and negative reviews in each split\n",
        "for split in splits:\n",
        "    # Count negative reviews (label = 0)\n",
        "    negative_count = sum(1 for item in ds_imdb[split] if item['label'] == 0)\n",
        "    # Count positive reviews (label = 1)\n",
        "    positive_count = sum(1 for item in ds_imdb[split] if item['label'] == 1)\n",
        "    total = len(ds_imdb[split])\n",
        "\n",
        "    print(f\"{split} split:\")\n",
        "    print(f\"  Negative reviews (label=0): {negative_count} ({negative_count/total:.2%})\")\n",
        "    print(f\"  Positive reviews (label=1): {positive_count} ({positive_count/total:.2%})\")\n",
        "    print()\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': Dataset({\n",
            "    features: ['text', 'label'],\n",
            "    num_rows: 500\n",
            "}), 'test': Dataset({\n",
            "    features: ['text', 'label'],\n",
            "    num_rows: 500\n",
            "})}\n",
            "train split:\n",
            "  Negative reviews (label=0): 254 (50.80%)\n",
            "  Positive reviews (label=1): 246 (49.20%)\n",
            "\n",
            "test split:\n",
            "  Negative reviews (label=0): 254 (50.80%)\n",
            "  Positive reviews (label=1): 246 (49.20%)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Model and Tokenizing the dataset"
      ],
      "metadata": {
        "id": "XsXaWaafdZDc"
      },
      "id": "XsXaWaafdZDc"
    },
    {
      "cell_type": "code",
      "source": [
        "#Models\n",
        "# Very good documentation  https://huggingface.co/transformers/v4.2.2/training.html\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "def return_model(model_name = \"distilbert-base-uncased\"):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},  # For converting predictions to strings\n",
        "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},)\n",
        "    return model\n",
        "\n",
        "model = return_model()\n",
        "print(model)\n",
        "\n",
        "#Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\"\"\"\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "tokenized_ds = {}\n",
        "for split in splits:\n",
        "    tokenized_ds[split] = ds_imdb[split].map(preprocess_function, batched=True)\n",
        "\n",
        "tokenized_test = ds_imdb[\"test\"].map(preprocess_function, batched=True)\n",
        "\n",
        "# Check that we tokenized the examples properly\n",
        "assert tokenized_ds[\"train\"][0][\"input_ids\"][:5] == [101, 2045, 2003, 2053, 7189]\n",
        "\n",
        "# Show the first example of the tokenized training set\n",
        "print(tokenized_ds[\"train\"][0][\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1d-4rKodcSV",
        "outputId": "2a8feff3-7d89-46d9-95d8-8fdadfced04a"
      },
      "id": "e1d-4rKodcSV",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlock(\n",
            "          (attention): DistilBertSdpaAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "[101, 2045, 2003, 2053, 7189, 2012, 2035, 2090, 3481, 3771, 1998, 6337, 2099, 2021, 1996, 2755, 2008, 2119, 2024, 2610, 2186, 2055, 6355, 6997, 1012, 6337, 2099, 3504, 15594, 2100, 1010, 3481, 3771, 3504, 4438, 1012, 6337, 2099, 14811, 2024, 3243, 3722, 1012, 3481, 3771, 1005, 1055, 5436, 2024, 2521, 2062, 8552, 1012, 1012, 1012, 3481, 3771, 3504, 2062, 2066, 3539, 8343, 1010, 2065, 2057, 2031, 2000, 3962, 12319, 1012, 1012, 1012, 1996, 2364, 2839, 2003, 5410, 1998, 6881, 2080, 1010, 2021, 2031, 1000, 17936, 6767, 7054, 3401, 1000, 1012, 2111, 2066, 2000, 12826, 1010, 2000, 3648, 1010, 2000, 16157, 1012, 2129, 2055, 2074, 9107, 1029, 6057, 2518, 2205, 1010, 2111, 3015, 3481, 3771, 3504, 2137, 2021, 1010, 2006, 1996, 2060, 2192, 1010, 9177, 2027, 9544, 2137, 2186, 1006, 999, 999, 999, 1007, 1012, 2672, 2009, 1005, 1055, 1996, 2653, 1010, 2030, 1996, 4382, 1010, 2021, 1045, 2228, 2023, 2186, 2003, 2062, 2394, 2084, 2137, 1012, 2011, 1996, 2126, 1010, 1996, 5889, 2024, 2428, 2204, 1998, 6057, 1012, 1996, 3772, 2003, 2025, 23105, 2012, 2035, 1012, 1012, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the pre trained (model not trained yet)"
      ],
      "metadata": {
        "id": "GaZhkhrndeML"
      },
      "id": "GaZhkhrndeML"
    },
    {
      "cell_type": "code",
      "id": "f28c4a78",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-27T02:39:43.944553Z",
          "start_time": "2025-04-27T02:38:59.769130Z"
        },
        "id": "f28c4a78"
      },
      "source": [
        "\"\"\"\n",
        "My own version\n",
        "\"\"\"\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make predictions\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(tokenized_test)):\n",
        "        inputs = {\n",
        "            \"input_ids\": torch.tensor(tokenized_test[i][\"input_ids\"]).unsqueeze(0),\n",
        "            \"attention_mask\": torch.tensor(tokenized_test[i][\"attention_mask\"]).unsqueeze(0),\n",
        "        }\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_label = torch.argmax(logits, dim=1).item()\n",
        "        predictions.append(predicted_label)\n",
        "        true_labels.append(tokenized_test[i][\"label\"])\n",
        "\n",
        "# Evaluate the predictions\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "f1_result = f1_score(true_labels, predictions)\n",
        "\n",
        "print()\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 result: {f1_result}\")\n",
        "\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "# Display the confusion matrix\n",
        "labels = [\"Negative\", \"Positive\"]  # IMDB sentiment labels\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "disp.plot(ax=ax)\n",
        "plt.title(\"Confusion Matrix for IMDB Sentiment Classification\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Lightweight Fine-Tuning"
      ],
      "metadata": {
        "id": "4eWNSY95jGT-"
      },
      "id": "4eWNSY95jGT-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the PEFT model"
      ],
      "metadata": {
        "id": "mAduGFabjfHC"
      },
      "id": "mAduGFabjfHC"
    },
    {
      "cell_type": "code",
      "id": "019b9f55",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-26T03:29:42.743968Z",
          "start_time": "2025-04-26T03:29:05.865452Z"
        },
        "id": "019b9f55"
      },
      "source": [
        "# Unfreeze all the model parameters.\n",
        "# Hint: Check the documentation at https://huggingface.co/transformers/v4.2.2/training.html\n",
        "model = return_model()\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "# Helper function to find all linear layer names\n",
        "def find_linear_layers(model):\n",
        "    linear_layers = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            linear_layers.append(name)\n",
        "    return linear_layers\n",
        "\n",
        "# Get the actual linear layer names in your model\n",
        "linear_layers = find_linear_layers(model)\n",
        "print(\"Linear layers found:\", linear_layers)\n",
        "print(model)\n",
        "from peft import LoraConfig\n",
        "# https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\n",
        "# https://huggingface.co/docs/peft/main/en/task_guides/image_classification_lora\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    # Use a more general approach to find linear layers\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"lin1\", \"lin2\", \"classifier\"],\n",
        "    modules_to_save=[\"classifier\"],\n",
        "    )\n",
        "\n",
        "from peft import get_peft_model\n",
        "lora_model = get_peft_model(model, config)\n",
        "\n",
        "lora_model.print_trainable_parameters()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the PEFT model"
      ],
      "metadata": {
        "id": "foDnfilg4uHw"
      },
      "id": "foDnfilg4uHw"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=  TrainingArguments(\n",
        "      f\"{model_name}-finetuned-lora-101\",\n",
        "      remove_unused_columns=False,\n",
        "      eval_strategy=\"epoch\",\n",
        "      save_strategy=\"epoch\",\n",
        "      learning_rate=5e-3,\n",
        "      per_device_train_batch_size= 128,\n",
        "      per_device_eval_batch_size= 128,\n",
        "      gradient_accumulation_steps=4,\n",
        "      fp16=True,\n",
        "      num_train_epochs=5,\n",
        "      logging_steps=10,\n",
        "      load_best_model_at_end=True,\n",
        "      metric_for_best_model=\"accuracy\",\n",
        "      push_to_hub=False,\n",
        "      label_names=[\"labels\"],\n",
        "      report_to=\"none\",\n",
        "    ),\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        "    #tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "train_results = trainer.train()\n"
      ],
      "metadata": {
        "id": "wP392H5d4xZq"
      },
      "id": "wP392H5d4xZq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the PEFT model"
      ],
      "metadata": {
        "id": "QjYIJHRI4yM0"
      },
      "id": "QjYIJHRI4yM0"
    },
    {
      "cell_type": "code",
      "source": [
        "model_trained = f\"{model_name}-finetuned-lora-101\")\n",
        "\n",
        "lora_model.save_pretrained(model_trained)\n",
        "from peft import AutoPeftModelForSequenceClassification, PeftModelForSequenceClassification\n",
        "lora_model = PeftModelForSequenceClassification.from_pretrained(model_trained)"
      ],
      "metadata": {
        "id": "Hu026fUw40sk"
      },
      "id": "Hu026fUw40sk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4d52a229",
      "metadata": {
        "id": "4d52a229"
      },
      "source": [
        "## Performing Parameter-Efficient Fine-Tuning\n",
        "\n",
        "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47abf88",
      "metadata": {
        "id": "b47abf88"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a7e8a663",
      "metadata": {
        "id": "a7e8a663"
      },
      "source": [
        "###  ⚠️ IMPORTANT ⚠️\n",
        "\n",
        "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
        "Ensure you save it in /tmp always."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa7fe003",
      "metadata": {
        "id": "fa7fe003"
      },
      "outputs": [],
      "source": [
        "# Saving the model\n",
        "model.save(\"/tmp/your_model_name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "615b12c6",
      "metadata": {
        "id": "615b12c6"
      },
      "source": [
        "## Performing Inference with a PEFT Model\n",
        "\n",
        "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863ec66e",
      "metadata": {
        "id": "863ec66e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3a8147",
      "metadata": {
        "id": "bc3a8147"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc96905a",
      "metadata": {
        "id": "bc96905a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "866ab28c",
      "metadata": {
        "id": "866ab28c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a32e4e",
      "metadata": {
        "id": "f9a32e4e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}