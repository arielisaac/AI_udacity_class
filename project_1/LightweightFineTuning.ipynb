{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "f551c63a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T18:25:45.388236Z",
     "start_time": "2025-04-22T18:25:40.779654Z"
    }
   },
   "source": [
    "# ModernBERT blog: https://www.answer.ai/posts/2024-12-19-modernbert.html\n",
    "# Philipp Schmid's blog on Fine-Tuning open LLMs: https://www.philschmid.de/fine-tune-llms-in-2025\n",
    "\n",
    "# https://learn.udacity.com/nanodegrees/nd101-ent-ai-swe-indeed/parts/cd13303/lessons/786df5de-95ad-4e0d-be51-cc8a1c1e40fe/concepts/ed4cd691-b999-454e-b715-a603fb2aeeb5?lesson_tab=lesson\n",
    "from peft import LoraConfig\n",
    "config = LoraConfig()\n",
    "# https://huggingface.co/docs/peft/main/en/conceptual_guides/lora"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "4935cb4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:24:22.036105Z",
     "start_time": "2025-04-26T03:24:19.063374Z"
    }
   },
   "source": [
    "import torch\n",
    "# This is to improve performance on M1 Macs\n",
    "def get_best_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_best_device()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sms_spam\", split=[\"train\"])[0]\n",
    "\n",
    "# Split the dataset into train and test sets (assuming `dataset` is a Hugging Face Dataset object)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "#print(dataset[:5])\n",
    "\n",
    "\"\"\"\n",
    "Convert the dataset to a pandas DataFrame\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the entire dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "# Display the first 10 rows\n",
    "print(df.head(10))\n",
    "\n",
    "# Or for more detailed information\n",
    "display(df.head(10))  # Works in Jupyter notebooks\n",
    "\n",
    "\n",
    "# for entry in dataset.select(range(3)):\n",
    "#     sms = entry[\"sms\"]\n",
    "#     label = entry[\"label\"]\n",
    "#     print(f\"label={label}, sms={sms}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 sms  label\n",
      "0  Go until jurong point, crazy.. Available only ...      0\n",
      "1                    Ok lar... Joking wif u oni...\\n      0\n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...      1\n",
      "3  U dun say so early hor... U c already then say...      0\n",
      "4  Nah I don't think he goes to usf, he lives aro...      0\n",
      "5  FreeMsg Hey there darling it's been 3 week's n...      1\n",
      "6  Even my brother is not like to speak with me. ...      0\n",
      "7  As per your request 'Melle Melle (Oru Minnamin...      0\n",
      "8  WINNER!! As a valued network customer you have...      1\n",
      "9  Had your mobile 11 months or more? U R entitle...      1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                 sms  label\n",
       "0  Go until jurong point, crazy.. Available only ...      0\n",
       "1                    Ok lar... Joking wif u oni...\\n      0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...      1\n",
       "3  U dun say so early hor... U c already then say...      0\n",
       "4  Nah I don't think he goes to usf, he lives aro...      0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...      1\n",
       "6  Even my brother is not like to speak with me. ...      0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...      0\n",
       "8  WINNER!! As a valued network customer you have...      1\n",
       "9  Had your mobile 11 months or more? U R entitle...      1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "f28c4a78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:46:04.305950Z",
     "start_time": "2025-04-26T03:45:18.491969Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "My own version\n",
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load the train and test splits of the imdb dataset\n",
    "splits = [\"train\", \"test\"]\n",
    "ds_imdb = {split: ds for split, ds in zip(splits, load_dataset(\"imdb\", split=splits))}\n",
    "\n",
    "# Thin out the dataset to make it run faster for this example\n",
    "for split in splits:\n",
    "    ds_imdb[split] = ds_imdb[split].shuffle(seed=42).select(range(500))\n",
    "\n",
    "\n",
    "#Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\"\"\"\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds_imdb[split].map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_test = ds_imdb[\"test\"].map(preprocess_function, batched=True)\n",
    "\n",
    "# Check that we tokenized the examples properly\n",
    "assert tokenized_ds[\"train\"][0][\"input_ids\"][:5] == [101, 2045, 2003, 2053, 7189]\n",
    "\n",
    "# Show the first example of the tokenized training set\n",
    "print(tokenized_ds[\"train\"][0][\"input_ids\"])\n",
    "\n",
    "\n",
    "#Models\n",
    "# Very good documentation  https://huggingface.co/transformers/v4.2.2/training.html\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},  # For converting predictions to strings\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(tokenized_test)):\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(tokenized_test[i][\"input_ids\"]).unsqueeze(0),\n",
    "            \"attention_mask\": torch.tensor(tokenized_test[i][\"attention_mask\"]).unsqueeze(0),\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()\n",
    "        predictions.append(predicted_label)\n",
    "        true_labels.append(tokenized_test[i][\"label\"])\n",
    "\n",
    "# Evaluate the predictions\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2045, 2003, 2053, 7189, 2012, 2035, 2090, 3481, 3771, 1998, 6337, 2099, 2021, 1996, 2755, 2008, 2119, 2024, 2610, 2186, 2055, 6355, 6997, 1012, 6337, 2099, 3504, 15594, 2100, 1010, 3481, 3771, 3504, 4438, 1012, 6337, 2099, 14811, 2024, 3243, 3722, 1012, 3481, 3771, 1005, 1055, 5436, 2024, 2521, 2062, 8552, 1012, 1012, 1012, 3481, 3771, 3504, 2062, 2066, 3539, 8343, 1010, 2065, 2057, 2031, 2000, 3962, 12319, 1012, 1012, 1012, 1996, 2364, 2839, 2003, 5410, 1998, 6881, 2080, 1010, 2021, 2031, 1000, 17936, 6767, 7054, 3401, 1000, 1012, 2111, 2066, 2000, 12826, 1010, 2000, 3648, 1010, 2000, 16157, 1012, 2129, 2055, 2074, 9107, 1029, 6057, 2518, 2205, 1010, 2111, 3015, 3481, 3771, 3504, 2137, 2021, 1010, 2006, 1996, 2060, 2192, 1010, 9177, 2027, 9544, 2137, 2186, 1006, 999, 999, 999, 1007, 1012, 2672, 2009, 1005, 1055, 1996, 2653, 1010, 2030, 1996, 4382, 1010, 2021, 1045, 2228, 2023, 2186, 2003, 2062, 2394, 2084, 2137, 1012, 2011, 1996, 2126, 1010, 1996, 5889, 2024, 2428, 2204, 1998, 6057, 1012, 1996, 3772, 2003, 2025, 23105, 2012, 2035, 1012, 1012, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Accuracy: 0.508\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "019b9f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:29:42.743968Z",
     "start_time": "2025-04-26T03:29:05.865452Z"
    }
   },
   "source": [
    "# These are just tests to see if I can get the model to work\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tokenize the test data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sms'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(tokenized_test)):\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(tokenized_test[i][\"input_ids\"]).unsqueeze(0),\n",
    "            \"attention_mask\": torch.tensor(tokenized_test[i][\"attention_mask\"]).unsqueeze(0),\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.last_hidden_state.mean(dim=1)  # Simplified for classification\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()\n",
    "        predictions.append(predicted_label)\n",
    "        true_labels.append(test_dataset[i][\"label\"])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Evaluate the predictions\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(outputs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "BaseModelOutput(last_hidden_state=tensor([[[-1.8296e-01, -7.4054e-02,  5.0267e-02,  ..., -1.1261e-01,\n",
      "           4.4493e-01,  4.0941e-01],\n",
      "         [ 7.0642e-04,  1.4825e-01,  3.4328e-01,  ..., -8.6040e-02,\n",
      "           6.9475e-01,  4.3353e-02],\n",
      "         [-5.0721e-01,  5.3085e-01,  3.7163e-01,  ..., -5.6288e-01,\n",
      "           1.3756e-01,  2.8475e-01],\n",
      "         ...,\n",
      "         [-4.2251e-01,  5.7315e-02,  2.4338e-01,  ..., -1.5223e-01,\n",
      "           2.4462e-01,  6.4155e-01],\n",
      "         [-4.9384e-01, -1.8895e-01,  1.2641e-01,  ...,  6.3240e-02,\n",
      "           3.6913e-01, -5.8252e-02],\n",
      "         [ 8.3269e-01,  2.4948e-01, -4.5440e-01,  ...,  1.1998e-01,\n",
      "          -3.9257e-01, -2.7785e-01]]], grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model.save(\"/tmp/your_model_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
